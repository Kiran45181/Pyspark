{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKNW0igCXd5OQZSvkiPwGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kiran45181/Pyspark/blob/main/Advanced%20Spark%20with%20java%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Advanced Spark with Java - Data Engineering Use Case Study"
      ],
      "metadata": {
        "id": "xNpomAWt3wNp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "niwf7W4vuofT",
        "outputId": "c1c0887b-b6dc-47c3-fec9-79df0654ec4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,918 kB]\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,773 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,161 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,207 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,103 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,270 kB]\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,575 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,518 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,290 kB]\n",
            "Fetched 34.2 MB in 10s (3,574 kB/s)\n",
            "Reading package lists...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jdk-headless is already the newest version (11.0.28+6-1ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "tar: spark-3.5.0-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "mv: cannot stat 'spark-3.5.0-bin-hadoop3': No such file or directory\n",
            "Collecting pyspark==3.5.0\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425346 sha256=cc3fc3d0dbc8255002fb40ff221d750fad0cd12f8f30d0c38c66af42dd4f0e6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/df/61/8c121f50c3cffd77f8178180dd232d90b3b99d1bd61fb6d6be\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.1\n",
            "    Uninstalling pyspark-3.5.1:\n",
            "      Successfully uninstalled pyspark-3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Update and install Java\n",
        "!apt-get update -q\n",
        "!apt-get install -y openjdk-11-jdk-headless\n",
        "\n",
        "# Download and install Spark 3.5.0\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!mv spark-3.5.0-bin-hadoop3 /usr/local/spark\n",
        "\n",
        "# Set environment variables\n",
        "!export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
        "!export PATH=$JAVA_HOME/bin:/usr/local/spark/bin:$PATH\n",
        "\n",
        "# Install PySpark\n",
        "!pip install pyspark==3.5.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
        "os.environ[\"PATH\"] += \":/usr/local/spark/bin:/usr/local/spark/sbin\""
      ],
      "metadata": {
        "id": "Qg1A7iPCuqnD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_qxSJ6Zvv4a",
        "outputId": "3b4c79b9-04c0-4c36-de03-734624768eb7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.5.0 in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os, shutil\n",
        "\n",
        "# Create target folder\n",
        "os.makedirs(\"/content/data/csv\", exist_ok=True)\n",
        "\n",
        "# Upload multiple CSV files (select all 8 files)\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Save uploaded files to /content/data/csv\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, f\"/content/data/csv/{filename}\")\n",
        "\n",
        "print(\"Files uploaded to /content/data/csv/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "omLaG4CZvyX8",
        "outputId": "80dfa6b8-7e1b-4451-ca40-b283d9d7f161"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9580d2fe-0851-4353-ad0f-0166ba2c784d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9580d2fe-0851-4353-ad0f-0166ba2c784d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving customers.csv to customers.csv\n",
            "Saving employees.csv to employees.csv\n",
            "Saving offices.csv to offices.csv\n",
            "Saving orderdetails.csv to orderdetails.csv\n",
            "Saving orders.csv to orders.csv\n",
            "Saving payments.csv to payments.csv\n",
            "Saving productlines.csv to productlines.csv\n",
            "Saving products.csv to products.csv\n",
            "Files uploaded to /content/data/csv/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "csv_folder = \"/content/data/csv\"\n",
        "parquet_folder = \"/content/data/parquet\"\n",
        "os.makedirs(parquet_folder, exist_ok=True)\n",
        "\n",
        "# Convert all CSV files to Parquet\n",
        "for file in os.listdir(csv_folder):\n",
        "    if file.endswith(\".csv\"):\n",
        "        df = pd.read_csv(os.path.join(csv_folder, file))\n",
        "        parquet_file = file.replace(\".csv\", \".parquet\")\n",
        "        df.to_parquet(os.path.join(parquet_folder, parquet_file), engine='pyarrow')\n",
        "        print(f\"Converted {file} -> {parquet_file}\")\n",
        "\n",
        "print(\"✅ All CSV files converted to Parquet format.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1zc4fQAv4kZ",
        "outputId": "14cab2ca-44af-4009-cf34-813de64642b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted orderdetails.csv -> orderdetails.parquet\n",
            "Converted products.csv -> products.parquet\n",
            "Converted employees.csv -> employees.parquet\n",
            "Converted orders.csv -> orders.parquet\n",
            "Converted productlines.csv -> productlines.parquet\n",
            "Converted customers.csv -> customers.parquet\n",
            "Converted payments.csv -> payments.parquet\n",
            "Converted offices.csv -> offices.parquet\n",
            "✅ All CSV files converted to Parquet format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/data/parquet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co_HoAuqwQbS",
        "outputId": "3c6127a5-77f0-459f-c491-33eab40a1588"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customers.parquet  offices.parquet\t orders.parquet    productlines.parquet\n",
            "employees.parquet  orderdetails.parquet  payments.parquet  products.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf spark-3.5.0-bin-hadoop3*\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "!mv spark-3.5.0-bin-hadoop3 /usr/local/spark\n"
      ],
      "metadata": {
        "id": "NKofBVXwwhVp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/spark\"\n",
        "os.environ[\"PATH\"] += \":/usr/local/spark/bin\"\n"
      ],
      "metadata": {
        "id": "M3OmOJ0vxc1D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark\n"
      ],
      "metadata": {
        "id": "5TEijbFNxfVs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ClassicModelsAnalysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Version:\", spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-MGaaVhxhnT",
        "outputId": "bf5c2da6-e19b-4189-fc01-bd278290d12c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Version: 3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all tables into Spark DataFrames\n",
        "customers = spark.read.parquet(\"/content/data/parquet/customers.parquet\")\n",
        "employees = spark.read.parquet(\"/content/data/parquet/employees.parquet\")\n",
        "offices = spark.read.parquet(\"/content/data/parquet/offices.parquet\")\n",
        "orders = spark.read.parquet(\"/content/data/parquet/orders.parquet\")\n",
        "orderdetails = spark.read.parquet(\"/content/data/parquet/orderdetails.parquet\")\n",
        "payments = spark.read.parquet(\"/content/data/parquet/payments.parquet\")\n",
        "products = spark.read.parquet(\"/content/data/parquet/products.parquet\")\n",
        "productlines = spark.read.parquet(\"/content/data/parquet/productlines.parquet\")\n",
        "\n",
        "# Register as Temp Views for SQL\n",
        "customers.createOrReplaceTempView(\"customers\")\n",
        "employees.createOrReplaceTempView(\"employees\")\n",
        "offices.createOrReplaceTempView(\"offices\")\n",
        "orders.createOrReplaceTempView(\"orders\")\n",
        "orderdetails.createOrReplaceTempView(\"orderdetails\")\n",
        "payments.createOrReplaceTempView(\"payments\")\n",
        "products.createOrReplaceTempView(\"products\")\n",
        "productlines.createOrReplaceTempView(\"productlines\")\n"
      ],
      "metadata": {
        "id": "MSiiV2V7wSoH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Group products by productLine and calculate aggregations\n",
        "products_summary = products.groupBy(\"productLine\").agg(\n",
        "    F.count(\"*\").alias(\"totalProducts\"),\n",
        "    F.avg(\"MSRP\").alias(\"avgMSRP\"),\n",
        "    F.max(\"MSRP\").alias(\"maxMSRP\")\n",
        ")\n",
        "\n",
        "# Save to Parquet\n",
        "output_path = \"/content/output/processed/products_summary.parquet\"\n",
        "products_summary.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"✅ Products Summary saved to: {output_path}\")\n",
        "products_summary.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GqriL8U3kMa",
        "outputId": "855f9dec-e068-4803-ee2d-23083ae87ac2"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Products Summary saved to: /content/output/processed/products_summary.parquet\n",
            "+----------------+-------------+------------------+-------+\n",
            "|productLine     |totalProducts|avgMSRP           |maxMSRP|\n",
            "+----------------+-------------+------------------+-------+\n",
            "|Motorcycles     |13           |97.17846153846153 |193.66 |\n",
            "|Vintage Cars    |24           |87.09583333333332 |170.0  |\n",
            "|Ships           |9            |86.56333333333333 |122.89 |\n",
            "|Trucks and Buses|11           |103.18363636363637|136.67 |\n",
            "|Classic Cars    |38           |118.02105263157895|214.3  |\n",
            "|Trains          |3            |73.85333333333334 |100.84 |\n",
            "|Planes          |12           |89.51583333333336 |157.69 |\n",
            "+----------------+-------------+------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 2"
      ],
      "metadata": {
        "id": "xMKIgJHayaMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 10 products by total quantity sold\n",
        "top_products_qty = spark.sql(\"\"\"\n",
        "SELECT p.productName,\n",
        "       SUM(od.quantityOrdered) AS total_quantity\n",
        "FROM orderdetails od\n",
        "JOIN products p ON od.productCode = p.productCode\n",
        "GROUP BY p.productName\n",
        "ORDER BY total_quantity DESC\n",
        "LIMIT 10\n",
        "\"\"\")\n",
        "\n",
        "top_products_qty.show(10, truncate=False)\n",
        "\n",
        "# Save result to Parquet\n",
        "output_path = \"/content/output/processed/top_10_products_quantity.parquet\"\n",
        "top_products_qty.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(f\"Saved Top 10 Products (Quantity) to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afojCLc0yZjn",
        "outputId": "bde12388-8893-4c62-c86e-b5b6391707ac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------+--------------+\n",
            "|productName                            |total_quantity|\n",
            "+---------------------------------------+--------------+\n",
            "|1992 Ferrari 360 Spider red            |1808          |\n",
            "|1937 Lincoln Berline                   |1111          |\n",
            "|American Airlines: MD-11S              |1085          |\n",
            "|1941 Chevrolet Special Deluxe Cabriolet|1076          |\n",
            "|1930 Buick Marquette Phaeton           |1074          |\n",
            "|1940s Ford truck                       |1061          |\n",
            "|1969 Harley Davidson Ultimate Chopper  |1057          |\n",
            "|1957 Chevy Pickup                      |1056          |\n",
            "|1964 Mercedes Tour Bus                 |1053          |\n",
            "|1956 Porsche 356A Coupe                |1052          |\n",
            "+---------------------------------------+--------------+\n",
            "\n",
            "Saved Top 10 Products (Quantity) to /content/output/processed/top_10_products_quantity.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Product-wise revenue (priceEach * quantityOrdered)\n",
        "product_revenue = spark.sql(\"\"\"\n",
        "SELECT p.productName,\n",
        "       SUM(od.quantityOrdered * od.priceEach) AS total_revenue\n",
        "FROM orderdetails od\n",
        "JOIN products p ON od.productCode = p.productCode\n",
        "GROUP BY p.productName\n",
        "ORDER BY total_revenue DESC\n",
        "\"\"\")\n",
        "\n",
        "product_revenue.show(10, truncate=False)\n",
        "\n",
        "# Save to Parquet\n",
        "output_path = \"/content/output/processed/product_revenue.parquet\"\n",
        "product_revenue.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(f\"Saved Product Revenue to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7QPVMuxwbO5",
        "outputId": "197c8343-6d9c-4572-eda0-d705239cf1ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+------------------+\n",
            "|productName                         |total_revenue     |\n",
            "+------------------------------------+------------------+\n",
            "|1992 Ferrari 360 Spider red         |276839.98         |\n",
            "|2001 Ferrari Enzo                   |190755.86         |\n",
            "|1952 Alpine Renault 1300            |190017.95999999996|\n",
            "|2003 Harley-Davidson Eagle Drag Bike|170685.99999999997|\n",
            "|1968 Ford Mustang                   |161531.47999999992|\n",
            "|1969 Ford Falcon                    |152543.02         |\n",
            "|1980s Black Hawk Helicopter         |144959.90999999997|\n",
            "|1998 Chrysler Plymouth Prowler      |142530.62999999998|\n",
            "|1917 Grand Touring Sedan            |140535.60000000003|\n",
            "|2002 Suzuki XREO                    |135767.03000000003|\n",
            "+------------------------------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Saved Product Revenue to /content/output/processed/product_revenue.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Average order value per customer\n",
        "avg_order_value = spark.sql(\"\"\"\n",
        "SELECT c.customerNumber,\n",
        "       c.customerName,\n",
        "       AVG(od.quantityOrdered * od.priceEach) AS avg_order_value\n",
        "FROM orders o\n",
        "JOIN customers c ON o.customerNumber = c.customerNumber\n",
        "JOIN orderdetails od ON o.orderNumber = od.orderNumber\n",
        "GROUP BY c.customerNumber, c.customerName\n",
        "ORDER BY avg_order_value DESC\n",
        "\"\"\")\n",
        "\n",
        "avg_order_value.show(10, truncate=False)\n",
        "\n",
        "# Save to Parquet\n",
        "output_path = \"/content/output/processed/avg_order_value_per_customer.parquet\"\n",
        "avg_order_value.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(f\"Saved Average Order Value per Customer to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zEvzUHRykUB",
        "outputId": "3c358f5c-bce7-49c7-eafc-fe328d667adc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------------------+------------------+\n",
            "|customerNumber|customerName               |avg_order_value   |\n",
            "+--------------+---------------------------+------------------+\n",
            "|455           |Super Scale Inc.           |4139.920588235294 |\n",
            "|209           |Mini Caravy                |3992.595789473684 |\n",
            "|328           |Tekni Collectables Inc.    |3895.5499999999993|\n",
            "|175           |Gift Depot Inc.            |3816.9851999999996|\n",
            "|172           |La Corne D'abondance, Co.  |3763.1965217391307|\n",
            "|151           |Muscle Machine Inc         |3706.540625       |\n",
            "|204           |Online Mini Collectables   |3705.1506666666664|\n",
            "|333           |Australian Gift Network, Co|3679.3439999999996|\n",
            "|201           |UK Collectables, Ltd.      |3676.2317241379315|\n",
            "|381           |Royale Belge               |3652.1475         |\n",
            "+--------------+---------------------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Saved Average Order Value per Customer to /content/output/processed/avg_order_value_per_customer.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Join orders -> customers -> payments\n",
        "order_payment = orders \\\n",
        "    .join(customers, \"customerNumber\", \"inner\") \\\n",
        "    .join(payments, \"customerNumber\", \"inner\")\n",
        "\n",
        "# Select and rename required columns\n",
        "order_payment_report = order_payment.select(\n",
        "    orders.orderNumber,\n",
        "    orders.orderDate,\n",
        "    customers.customerName,\n",
        "    payments.amount.alias(\"paymentAmount\"),\n",
        "    orders.status\n",
        ")\n",
        "\n",
        "# Save to Parquet\n",
        "output_path = \"/content/output/processed/order_payment_report.parquet\"\n",
        "order_payment_report.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"✅ Order Payment Report saved to: {output_path}\")\n",
        "order_payment_report.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCHsWW-43QTz",
        "outputId": "436f71b7-bd9f-49ba-d23d-933efdcd7b33"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Order Payment Report saved to: /content/output/processed/order_payment_report.parquet\n",
            "+-----------+----------+----------------------------+-------------+-------+\n",
            "|orderNumber|orderDate |customerName                |paymentAmount|status |\n",
            "+-----------+----------+----------------------------+-------------+-------+\n",
            "|10100      |2003-01-06|Online Diecast Creations Co.|55425.77     |Shipped|\n",
            "|10100      |2003-01-06|Online Diecast Creations Co.|10223.83     |Shipped|\n",
            "|10100      |2003-01-06|Online Diecast Creations Co.|50799.69     |Shipped|\n",
            "|10101      |2003-01-09|Blauer See Auto, Co.        |7466.32      |Shipped|\n",
            "|10101      |2003-01-09|Blauer See Auto, Co.        |33820.62     |Shipped|\n",
            "|10101      |2003-01-09|Blauer See Auto, Co.        |24101.81     |Shipped|\n",
            "|10101      |2003-01-09|Blauer See Auto, Co.        |10549.01     |Shipped|\n",
            "|10102      |2003-01-10|Vitachrome Inc.             |44400.5      |Shipped|\n",
            "|10102      |2003-01-10|Vitachrome Inc.             |5494.78      |Shipped|\n",
            "|10102      |2003-01-10|Vitachrome Inc.             |22602.36     |Shipped|\n",
            "+-----------+----------+----------------------------+-------------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 3 : Regional Sales Insights"
      ],
      "metadata": {
        "id": "RlCDKbSRysHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sales per region: join offices -> employees -> customers -> orders -> orderdetails\n",
        "sales_per_region = spark.sql(\"\"\"\n",
        "SELECT o.country,\n",
        "       o.city,\n",
        "       SUM(od.quantityOrdered * od.priceEach) AS total_sales\n",
        "FROM offices o\n",
        "JOIN employees e ON o.officeCode = e.officeCode\n",
        "JOIN customers c ON e.employeeNumber = c.salesRepEmployeeNumber\n",
        "JOIN orders ord ON c.customerNumber = ord.customerNumber\n",
        "JOIN orderdetails od ON ord.orderNumber = od.orderNumber\n",
        "GROUP BY o.country, o.city\n",
        "ORDER BY total_sales DESC\n",
        "\"\"\")\n",
        "\n",
        "sales_per_region.show(10, truncate=False)\n",
        "\n",
        "# Save to Parquet\n",
        "output_path = \"/content/output/processed/sales_per_region.parquet\"\n",
        "sales_per_region.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(f\"Saved Sales per Region to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJpCmgMuynJa",
        "outputId": "b90d7606-e899-4e65-8273-58f32298f616"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+------------------+\n",
            "|country  |city         |total_sales       |\n",
            "+---------+-------------+------------------+\n",
            "|France   |Paris        |3083761.5800000066|\n",
            "|UK       |London       |1436950.699999998 |\n",
            "|USA      |San Francisco|1429063.569999999 |\n",
            "|USA      |NYC          |1157589.7200000004|\n",
            "|Australia|Sydney       |1147176.35        |\n",
            "|USA      |Boston       |892538.6199999999 |\n",
            "|Japan    |Tokyo        |457110.0700000002 |\n",
            "+---------+-------------+------------------+\n",
            "\n",
            "Saved Sales per Region to /content/output/processed/sales_per_region.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Total revenue by country (customers + payments)\n",
        "revenue_by_country = spark.sql(\"\"\"\n",
        "SELECT c.country,\n",
        "       SUM(p.amount) AS total_revenue\n",
        "FROM customers c\n",
        "JOIN payments p ON c.customerNumber = p.customerNumber\n",
        "GROUP BY c.country\n",
        "ORDER BY total_revenue DESC\n",
        "\"\"\")\n",
        "\n",
        "revenue_by_country.show(10, truncate=False)\n",
        "\n",
        "# Save to Parquet\n",
        "output_path = \"/content/output/processed/revenue_by_country.parquet\"\n",
        "revenue_by_country.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(f\"Saved Revenue by Country to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVfvJONJy09Q",
        "outputId": "8d3f3c9f-eb0a-4253-d811-b3ce240458d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|country    |total_revenue     |\n",
            "+-----------+------------------+\n",
            "|USA        |3040029.5199999996|\n",
            "|Spain      |994438.5300000003 |\n",
            "|France     |965750.5800000001 |\n",
            "|Australia  |509385.81999999995|\n",
            "|New Zealand|392486.59         |\n",
            "|UK         |391503.89999999997|\n",
            "|Italy      |325254.55000000005|\n",
            "|Finland    |295149.35         |\n",
            "|Singapore  |261671.59999999998|\n",
            "|Canada     |205911.86         |\n",
            "+-----------+------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Saved Revenue by Country to /content/output/processed/revenue_by_country.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top performing offices by total sales\n",
        "top_offices = spark.sql(\"\"\"\n",
        "SELECT o.officeCode,\n",
        "       o.city,\n",
        "       o.country,\n",
        "       SUM(od.quantityOrdered * od.priceEach) AS total_sales\n",
        "FROM offices o\n",
        "JOIN employees e ON o.officeCode = e.officeCode\n",
        "JOIN customers c ON e.employeeNumber = c.salesRepEmployeeNumber\n",
        "JOIN orders ord ON c.customerNumber = ord.customerNumber\n",
        "JOIN orderdetails od ON ord.orderNumber = od.orderNumber\n",
        "GROUP BY o.officeCode, o.city, o.country\n",
        "ORDER BY total_sales DESC\n",
        "\"\"\")\n",
        "\n",
        "top_offices.show(10, truncate=False)\n",
        "\n",
        "# Save to Parquet\n",
        "output_path = \"/content/output/processed/top_offices.parquet\"\n",
        "top_offices.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(f\"Saved Top Performing Offices to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKRi0_8cy4JU",
        "outputId": "9344842b-1f95-4ea5-ceec-8977145156e0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+---------+------------------+\n",
            "|officeCode|city         |country  |total_sales       |\n",
            "+----------+-------------+---------+------------------+\n",
            "|4         |Paris        |France   |3083761.5800000066|\n",
            "|7         |London       |UK       |1436950.699999998 |\n",
            "|1         |San Francisco|USA      |1429063.569999999 |\n",
            "|3         |NYC          |USA      |1157589.7200000004|\n",
            "|6         |Sydney       |Australia|1147176.35        |\n",
            "|2         |Boston       |USA      |892538.6199999999 |\n",
            "|5         |Tokyo        |Japan    |457110.0700000002 |\n",
            "+----------+-------------+---------+------------------+\n",
            "\n",
            "Saved Top Performing Offices to /content/output/processed/top_offices.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Join offices -> employees -> customers -> payments\n",
        "customer_sales_office = offices \\\n",
        "    .join(employees, offices.officeCode == employees.officeCode, \"inner\") \\\n",
        "    .join(customers, employees.employeeNumber == customers.salesRepEmployeeNumber, \"inner\") \\\n",
        "    .join(payments, customers.customerNumber == payments.customerNumber, \"inner\")\n",
        "\n",
        "# Select only needed columns (avoid duplicate customerNumber)\n",
        "customer_sales_office = customer_sales_office.select(\n",
        "    offices.officeCode,\n",
        "    offices.city,\n",
        "    offices.country,\n",
        "    customers.customerNumber.alias(\"custNumber\"),\n",
        "    payments.amount\n",
        ")\n",
        "\n",
        "# Group by office info\n",
        "customer_sales_by_office = customer_sales_office.groupBy(\n",
        "    \"officeCode\",\n",
        "    \"city\",\n",
        "    \"country\"\n",
        ").agg(\n",
        "    F.countDistinct(\"custNumber\").alias(\"customerCount\"),\n",
        "    F.sum(\"amount\").alias(\"totalSales\")\n",
        ")\n",
        "\n",
        "# Save to Parquet\n",
        "output_path = \"/content/output/processed/customer_sales_by_office.parquet\"\n",
        "customer_sales_by_office.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"✅ Customer Sales by Office saved to: {output_path}\")\n",
        "customer_sales_by_office.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUD8CJmX21r8",
        "outputId": "177311eb-14e7-4394-f19b-b2e9853cbf6c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Customer Sales by Office saved to: /content/output/processed/customer_sales_by_office.parquet\n",
            "+----------+-------------+---------+-------------+------------------+\n",
            "|officeCode|city         |country  |customerCount|totalSales        |\n",
            "+----------+-------------+---------+-------------+------------------+\n",
            "|6         |Sydney       |Australia|10           |1007292.9800000001|\n",
            "|2         |Boston       |USA      |12           |835882.33         |\n",
            "|3         |NYC          |USA      |14           |1072619.47        |\n",
            "|7         |London       |UK       |17           |1324325.9         |\n",
            "|4         |Paris        |France   |28           |2819168.9000000004|\n",
            "|1         |San Francisco|USA      |12           |1337439.5799999998|\n",
            "|5         |Tokyo        |Japan    |5            |457110.07         |\n",
            "+----------+-------------+---------+-------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 4 : Performance Optimization"
      ],
      "metadata": {
        "id": "cWmjgV7WzFJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache frequently used DataFrames to avoid recomputation\n",
        "customers.cache()\n",
        "employees.cache()\n",
        "offices.cache()\n",
        "orders.cache()\n",
        "orderdetails.cache()\n",
        "products.cache()\n",
        "payments.cache()\n",
        "\n",
        "print(\"✅ Cached frequently used tables.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xzojPA1y6Oq",
        "outputId": "8549e8e7-289f-41c9-ede1-f6e35467c824"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cached frequently used tables.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Example: Join broadcasted offices to employees for faster execution\n",
        "broadcast_offices = broadcast(offices)\n",
        "\n",
        "emp_office_join = employees.join(broadcast_offices, \"officeCode\") \\\n",
        "                           .select(\"employeeNumber\", \"lastName\", \"city\", \"country\")\n",
        "\n",
        "emp_office_join.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y27TeewzzMjW",
        "outputId": "a9e22a11-469d-4b5e-d1ea-085b8087ee99"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------+-------------+---------+\n",
            "|employeeNumber| lastName|         city|  country|\n",
            "+--------------+---------+-------------+---------+\n",
            "|          1002|   Murphy|San Francisco|      USA|\n",
            "|          1056|Patterson|San Francisco|      USA|\n",
            "|          1076| Firrelli|San Francisco|      USA|\n",
            "|          1088|Patterson|       Sydney|Australia|\n",
            "|          1102|   Bondur|        Paris|   France|\n",
            "+--------------+---------+-------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lazy evaluation demo\n",
        "lazy_df = orders.filter(\"status = 'Shipped'\").select(\"orderNumber\")\n",
        "\n",
        "print(\"No computation yet (lazy)...\")\n",
        "\n",
        "# Trigger computation with an action\n",
        "print(\"Count of Shipped Orders:\", lazy_df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAh3i1kwzOlc",
        "outputId": "50333365-e351-45b7-bb85-28145644d0d1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No computation yet (lazy)...\n",
            "Count of Shipped Orders: 303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert orderdetails to RDD for custom aggregation using aggregateByKey\n",
        "order_rdd = orderdetails.rdd.map(lambda row: (row.orderNumber, row.quantityOrdered * row.priceEach))\n",
        "\n",
        "# Aggregate revenue per order\n",
        "agg_rdd = order_rdd.aggregateByKey(0,\n",
        "                                   lambda acc, x: acc + x,   # seqOp\n",
        "                                   lambda acc1, acc2: acc1 + acc2)  # combOp\n",
        "\n",
        "# Convert back to DataFrame\n",
        "order_revenue_df = agg_rdd.toDF([\"orderNumber\", \"totalRevenue\"])\n",
        "order_revenue_df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJCAiNrYzRlF",
        "outputId": "0f5f32f5-8c27-4e25-d168-0e93805f000d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|orderNumber|      totalRevenue|\n",
            "+-----------+------------------+\n",
            "|      10100|10223.829999999998|\n",
            "|      10101|          10549.01|\n",
            "|      10102|           5494.78|\n",
            "|      10103|50218.950000000004|\n",
            "|      10104|           40206.2|\n",
            "+-----------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/output/processed/optimized_order_revenue.parquet\"\n",
        "order_revenue_df.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(f\"Optimized Order Revenue saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOx_MHtLzVHe",
        "outputId": "313a0115-58d6-4b56-dfeb-91cd48bc516b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Order Revenue saved to /content/output/processed/optimized_order_revenue.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Join employees -> customers -> orders -> payments\n",
        "employee_sales = employees \\\n",
        "    .join(customers, employees.employeeNumber == customers.salesRepEmployeeNumber, \"inner\") \\\n",
        "    .join(orders, customers.customerNumber == orders.customerNumber, \"inner\") \\\n",
        "    .join(payments, customers.customerNumber == payments.customerNumber, \"inner\")\n",
        "\n",
        "# Aggregate: Total orders & total sales by employee\n",
        "employee_sales_summary = employee_sales.groupBy(\n",
        "    employees.employeeNumber,\n",
        "    employees.firstName,\n",
        "    employees.lastName\n",
        ").agg(\n",
        "    F.countDistinct(\"orderNumber\").alias(\"totalOrders\"),  # FIX: remove table prefix\n",
        "    F.sum(\"amount\").alias(\"totalSales\")                   # FIX: remove table prefix\n",
        ")\n",
        "\n",
        "# Combine first and last name into employeeName\n",
        "employee_sales_summary = employee_sales_summary.withColumn(\n",
        "    \"employeeName\",\n",
        "    F.concat_ws(\" \", F.col(\"firstName\"), F.col(\"lastName\"))\n",
        ").select(\n",
        "    \"employeeNumber\", \"employeeName\", \"totalOrders\", \"totalSales\"\n",
        ")\n",
        "\n",
        "# Save result\n",
        "output_path = \"/content/output/processed/employee_sales_summary.parquet\"\n",
        "employee_sales_summary.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"✅ Employee Performance Summary saved to: {output_path}\")\n",
        "employee_sales_summary.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52MhUHqT2gjY",
        "outputId": "14751623-fb38-4300-93ba-a01b394de4f1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Employee Performance Summary saved to: /content/output/processed/employee_sales_summary.parquet\n",
            "+--------------+---------------+-----------+------------------+\n",
            "|employeeNumber|employeeName   |totalOrders|totalSales        |\n",
            "+--------------+---------------+-----------+------------------+\n",
            "|1611          |Andy Fixter    |19         |2118017.4299999997|\n",
            "|1621          |Mami Nishi     |16         |1681538.9900000002|\n",
            "|1286          |Foon Yue Tseng |17         |1542942.88        |\n",
            "|1166          |Leslie Thompson|14         |869050.3400000001 |\n",
            "|1188          |Julie Firrelli |14         |948732.5700000001 |\n",
            "|1504          |Barry Jones    |25         |1866060.52        |\n",
            "|1612          |Peter Marsh    |19         |2022883.4000000001|\n",
            "|1323          |George Vanauf  |22         |1645493.9899999998|\n",
            "|1702          |Martin Gerard  |12         |955915.4999999999 |\n",
            "|1216          |Steve Patterson|18         |1355621.27        |\n",
            "+--------------+---------------+-----------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 5 : Code Structure & Submission"
      ],
      "metadata": {
        "id": "K6vx9so5zpYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir -p /content/src/java\n",
        "mkdir -p /content/build/java\n",
        "mkdir -p /content/output/processed\n"
      ],
      "metadata": {
        "id": "Wr9ikEPA0N3n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/src/java/OrderRevenueAnalysis.java\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "\n",
        "public class OrderRevenueAnalysis {\n",
        "    public static void main(String[] args) {\n",
        "        // Initialize Spark Session\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"OrderRevenueAnalysis\")\n",
        "                .config(\"spark.master\", \"local[*]\")   // Required for Colab\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Load Parquet data\n",
        "        Dataset<Row> orders = spark.read().parquet(\"/content/data/parquet/orders.parquet\");\n",
        "        Dataset<Row> orderdetails = spark.read().parquet(\"/content/data/parquet/orderdetails.parquet\");\n",
        "        Dataset<Row> products = spark.read().parquet(\"/content/data/parquet/products.parquet\");\n",
        "\n",
        "        // Join & calculate revenue\n",
        "        Dataset<Row> revenue = orderdetails.join(orders, \"orderNumber\")\n",
        "                .join(products, \"productCode\")\n",
        "                .groupBy(\"productName\")\n",
        "                .sum(\"quantityOrdered\")\n",
        "                .withColumnRenamed(\"sum(quantityOrdered)\", \"totalQuantity\");\n",
        "\n",
        "        // Save output\n",
        "        revenue.write().mode(\"overwrite\").parquet(\"/content/output/processed/java_order_revenue.parquet\");\n",
        "\n",
        "        System.out.println(\"Parquet written to /content/output/processed/java_order_revenue.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCkse0DnzX6N",
        "outputId": "6d2cb208-196d-4ec6-d314-25cd00b6b47a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/src/java/OrderRevenueAnalysis.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Set Spark classpath\n",
        "CLASS_PATH=$(echo /usr/local/spark/jars/*.jar | tr ' ' ':')\n",
        "\n",
        "# Compile Java program\n",
        "javac -cp \"$CLASS_PATH\" -d /content/build/java /content/src/java/OrderRevenueAnalysis.java\n",
        "\n",
        "# Run Java program\n",
        "java -cp \"/content/build/java:$CLASS_PATH\" OrderRevenueAnalysis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5Es2vYiTz6Cj",
        "outputId": "bd4e08d1-6430-4c6f-f985-90e09883a27d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet written to /content/output/processed/java_order_revenue.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:18:49 INFO SparkContext: Running Spark version 3.5.0\n",
            "25/08/06 05:18:49 INFO SparkContext: OS info Linux, 6.1.123+, amd64\n",
            "25/08/06 05:18:49 INFO SparkContext: Java version 11.0.28\n",
            "25/08/06 05:18:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:18:49 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:18:49 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:18:49 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:18:49 INFO SparkContext: Submitted application: OrderRevenueAnalysis\n",
            "25/08/06 05:18:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:18:49 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:18:49 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:18:50 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:18:50 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:18:50 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:18:50 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:18:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:18:50 INFO Utils: Successfully started service 'sparkDriver' on port 42457.\n",
            "25/08/06 05:18:50 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.5.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:18:50 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:18:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:18:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:18:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:18:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fb91a84b-0148-49e3-9fa7-85999c6fdb3f\n",
            "25/08/06 05:18:51 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:18:51 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:18:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:18:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 05:18:51 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 05:18:51 INFO Executor: Starting executor ID driver on host c3748e2df8f0\n",
            "25/08/06 05:18:51 INFO Executor: OS info Linux, 6.1.123+, amd64\n",
            "25/08/06 05:18:51 INFO Executor: Java version 11.0.28\n",
            "25/08/06 05:18:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:18:51 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4012d5bc for default.\n",
            "25/08/06 05:18:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40503.\n",
            "25/08/06 05:18:52 INFO NettyBlockTransferService: Server created on c3748e2df8f0:40503\n",
            "25/08/06 05:18:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:18:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c3748e2df8f0, 40503, None)\n",
            "25/08/06 05:18:52 INFO BlockManagerMasterEndpoint: Registering block manager c3748e2df8f0:40503 with 1767.6 MiB RAM, BlockManagerId(driver, c3748e2df8f0, 40503, None)\n",
            "25/08/06 05:18:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c3748e2df8f0, 40503, None)\n",
            "25/08/06 05:18:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c3748e2df8f0, 40503, None)\n",
            "25/08/06 05:18:52 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:18:52 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 05:18:54 INFO InMemoryFileIndex: It took 91 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:18:55 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:14\n",
            "25/08/06 05:18:55 INFO DAGScheduler: Got job 0 (parquet at OrderRevenueAnalysis.java:14) with 1 output partitions\n",
            "25/08/06 05:18:55 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at OrderRevenueAnalysis.java:14)\n",
            "25/08/06 05:18:55 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:18:55 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:18:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at OrderRevenueAnalysis.java:14), which has no missing parents\n",
            "25/08/06 05:18:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.8 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:18:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:18:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c3748e2df8f0:40503 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:18:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:18:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at OrderRevenueAnalysis.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:18:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:18:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 7751 bytes) \n",
            "25/08/06 05:18:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 05:18:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1899 bytes result sent to driver\n",
            "25/08/06 05:18:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 900 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:18:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:18:57 INFO DAGScheduler: ResultStage 0 (parquet at OrderRevenueAnalysis.java:14) finished in 1.310 s\n",
            "25/08/06 05:18:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:18:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 05:18:57 INFO DAGScheduler: Job 0 finished: parquet at OrderRevenueAnalysis.java:14, took 1.540969 s\n",
            "25/08/06 05:18:58 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:18:58 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:15\n",
            "25/08/06 05:18:58 INFO DAGScheduler: Got job 1 (parquet at OrderRevenueAnalysis.java:15) with 1 output partitions\n",
            "25/08/06 05:18:58 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at OrderRevenueAnalysis.java:15)\n",
            "25/08/06 05:18:58 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:18:58 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:18:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at OrderRevenueAnalysis.java:15), which has no missing parents\n",
            "25/08/06 05:18:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.8 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:18:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 05:18:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c3748e2df8f0:40503 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:18:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:18:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at OrderRevenueAnalysis.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:18:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:18:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 7757 bytes) \n",
            "25/08/06 05:18:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 05:18:58 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1899 bytes result sent to driver\n",
            "25/08/06 05:18:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 52 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:18:58 INFO DAGScheduler: ResultStage 1 (parquet at OrderRevenueAnalysis.java:15) finished in 0.081 s\n",
            "25/08/06 05:18:58 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:18:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:18:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 05:18:58 INFO DAGScheduler: Job 1 finished: parquet at OrderRevenueAnalysis.java:15, took 0.091685 s\n",
            "25/08/06 05:18:58 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:18:59 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:16\n",
            "25/08/06 05:18:59 INFO DAGScheduler: Got job 2 (parquet at OrderRevenueAnalysis.java:16) with 1 output partitions\n",
            "25/08/06 05:18:59 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at OrderRevenueAnalysis.java:16)\n",
            "25/08/06 05:18:59 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:18:59 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:18:59 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at OrderRevenueAnalysis.java:16), which has no missing parents\n",
            "25/08/06 05:18:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.8 KiB, free 1767.2 MiB)\n",
            "25/08/06 05:18:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.2 MiB)\n",
            "25/08/06 05:18:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c3748e2df8f0:40503 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:18:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:18:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at OrderRevenueAnalysis.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:18:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:18:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 7753 bytes) \n",
            "25/08/06 05:18:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 05:18:59 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2021 bytes result sent to driver\n",
            "25/08/06 05:18:59 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 71 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:18:59 INFO DAGScheduler: ResultStage 2 (parquet at OrderRevenueAnalysis.java:16) finished in 0.123 s\n",
            "25/08/06 05:18:59 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:18:59 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:18:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 05:18:59 INFO DAGScheduler: Job 2 finished: parquet at OrderRevenueAnalysis.java:16, took 0.139185 s\n",
            "25/08/06 05:19:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on c3748e2df8f0:40503 in memory (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:02 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c3748e2df8f0:40503 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:19:02 INFO BlockManagerInfo: Removed broadcast_0_piece0 on c3748e2df8f0:40503 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:19:02 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber),IsNotNull(productCode)\n",
            "25/08/06 05:19:02 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#14L),isnotnull(productCode#15)\n",
            "25/08/06 05:19:02 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 05:19:02 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#0L)\n",
            "25/08/06 05:19:02 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 05:19:02 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#24)\n",
            "25/08/06 05:19:03 INFO CodeGenerator: Code generated in 467.423152 ms\n",
            "25/08/06 05:19:03 INFO CodeGenerator: Code generated in 476.414279 ms\n",
            "25/08/06 05:19:03 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:19:03 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 KiB, free 1767.2 MiB)\n",
            "25/08/06 05:19:03 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.2 MiB)\n",
            "25/08/06 05:19:03 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c3748e2df8f0:40503 (size: 34.7 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:19:03 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:19:03 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:19:03 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c3748e2df8f0:40503 (size: 34.6 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:03 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:19:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:19:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:19:04 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:19:04 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[12] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:19:04 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.3 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:19:04 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:19:04 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on c3748e2df8f0:40503 (size: 6.3 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:04 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:19:04 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:19:04 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 8221 bytes) \n",
            "25/08/06 05:19:04 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:19:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 15.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:19:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:19:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on c3748e2df8f0:40503 (size: 6.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:04 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:19:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:19:04 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:19:04 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 8223 bytes) \n",
            "25/08/06 05:19:04 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 05:19:04 INFO CodeGenerator: Code generated in 60.67536 ms\n",
            "25/08/06 05:19:04 INFO CodeGenerator: Code generated in 64.648102 ms\n",
            "25/08/06 05:19:04 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders.parquet, range: 0-14618, partition values: [empty row]\n",
            "25/08/06 05:19:04 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products.parquet, range: 0-20163, partition values: [empty row]\n",
            "25/08/06 05:19:04 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 05:19:04 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 05:19:04 INFO ColumnIndexFilter: No offset index for column productCode is available; Unable to do filtering\n",
            "25/08/06 05:19:04 INFO ColumnIndexFilter: No offset index for column orderNumber is available; Unable to do filtering\n",
            "25/08/06 05:19:04 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 05:19:04 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 05:19:05 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 5505 bytes result sent to driver\n",
            "25/08/06 05:19:05 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3545 bytes result sent to driver\n",
            "25/08/06 05:19:05 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1158 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:19:05 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.190 s\n",
            "25/08/06 05:19:05 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:19:05 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:19:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 05:19:05 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.209593 s\n",
            "25/08/06 05:19:05 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1123 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:19:05 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 1.165 s\n",
            "25/08/06 05:19:05 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:19:05 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:19:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 05:19:05 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 1.222732 s\n",
            "25/08/06 05:19:05 INFO CodeGenerator: Code generated in 42.844186 ms\n",
            "25/08/06 05:19:05 INFO CodeGenerator: Code generated in 43.108568 ms\n",
            "25/08/06 05:19:05 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 1026.6 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:19:05 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:19:05 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on c3748e2df8f0:40503 (size: 3.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:05 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:19:05 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 MiB, free 1734.1 MiB)\n",
            "25/08/06 05:19:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber),IsNotNull(productCode)\n",
            "25/08/06 05:19:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#14L),isnotnull(productCode#15)\n",
            "25/08/06 05:19:05 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1734.1 MiB)\n",
            "25/08/06 05:19:05 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on c3748e2df8f0:40503 (size: 4.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:05 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:19:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber),IsNotNull(productCode)\n",
            "25/08/06 05:19:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#14L),isnotnull(productCode#15)\n",
            "25/08/06 05:19:06 INFO CodeGenerator: Code generated in 207.719921 ms\n",
            "25/08/06 05:19:06 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.3 KiB, free 1733.9 MiB)\n",
            "25/08/06 05:19:06 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1733.9 MiB)\n",
            "25/08/06 05:19:06 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on c3748e2df8f0:40503 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:06 INFO SparkContext: Created broadcast 9 from parquet at OrderRevenueAnalysis.java:26\n",
            "25/08/06 05:19:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:19:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on c3748e2df8f0:40503 in memory (size: 6.3 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:06 INFO BlockManagerInfo: Removed broadcast_6_piece0 on c3748e2df8f0:40503 in memory (size: 6.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:06 INFO DAGScheduler: Registering RDD 17 (parquet at OrderRevenueAnalysis.java:26) as input to shuffle 0\n",
            "25/08/06 05:19:06 INFO DAGScheduler: Got map stage job 5 (parquet at OrderRevenueAnalysis.java:26) with 1 output partitions\n",
            "25/08/06 05:19:06 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (parquet at OrderRevenueAnalysis.java:26)\n",
            "25/08/06 05:19:06 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:19:06 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:19:06 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[17] at parquet at OrderRevenueAnalysis.java:26), which has no missing parents\n",
            "25/08/06 05:19:06 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 54.1 KiB, free 1733.8 MiB)\n",
            "25/08/06 05:19:06 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 1733.8 MiB)\n",
            "25/08/06 05:19:06 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on c3748e2df8f0:40503 (size: 23.2 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:06 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:19:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[17] at parquet at OrderRevenueAnalysis.java:26) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:19:06 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:19:06 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 8216 bytes) \n",
            "25/08/06 05:19:06 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 05:19:06 INFO CodeGenerator: Code generated in 125.165512 ms\n",
            "25/08/06 05:19:06 INFO CodeGenerator: Code generated in 39.550064 ms\n",
            "25/08/06 05:19:06 INFO CodeGenerator: Code generated in 9.881812 ms\n",
            "25/08/06 05:19:06 INFO CodeGenerator: Code generated in 10.419304 ms\n",
            "25/08/06 05:19:06 INFO CodeGenerator: Code generated in 10.147043 ms\n",
            "25/08/06 05:19:06 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails.parquet, range: 0-26383, partition values: [empty row]\n",
            "25/08/06 05:19:06 INFO FilterCompat: Filtering using predicate: and(noteq(orderNumber, null), noteq(productCode, null))\n",
            "25/08/06 05:19:06 INFO ColumnIndexFilter: No offset index for column orderNumber is available; Unable to do filtering\n",
            "25/08/06 05:19:07 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4766 bytes result sent to driver\n",
            "25/08/06 05:19:07 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 618 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:19:07 INFO DAGScheduler: ShuffleMapStage 5 (parquet at OrderRevenueAnalysis.java:26) finished in 0.677 s\n",
            "25/08/06 05:19:07 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:19:07 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:19:07 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:19:07 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:19:07 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:19:07 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:19:07 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:19:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:19:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:19:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:19:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:19:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:19:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:19:07 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 05:19:07 INFO CodeGenerator: Code generated in 24.402869 ms\n",
            "25/08/06 05:19:07 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:26\n",
            "25/08/06 05:19:07 INFO DAGScheduler: Got job 6 (parquet at OrderRevenueAnalysis.java:26) with 1 output partitions\n",
            "25/08/06 05:19:07 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at OrderRevenueAnalysis.java:26)\n",
            "25/08/06 05:19:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "25/08/06 05:19:07 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:19:07 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[20] at parquet at OrderRevenueAnalysis.java:26), which has no missing parents\n",
            "25/08/06 05:19:07 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 250.1 KiB, free 1733.6 MiB)\n",
            "25/08/06 05:19:07 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 92.5 KiB, free 1733.5 MiB)\n",
            "25/08/06 05:19:07 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on c3748e2df8f0:40503 (size: 92.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:19:07 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:19:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[20] at parquet at OrderRevenueAnalysis.java:26) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:19:07 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:19:07 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (c3748e2df8f0, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
            "25/08/06 05:19:07 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "25/08/06 05:19:07 INFO ShuffleBlockFetcherIterator: Getting 1 (8.8 KiB) non-empty blocks including 1 (8.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:19:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms\n",
            "25/08/06 05:19:07 INFO CodeGenerator: Code generated in 32.118225 ms\n",
            "25/08/06 05:19:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:19:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:19:07 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:19:07 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:19:07 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:19:07 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:19:07 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:19:07 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:19:07 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:19:07 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalQuantity\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productName (STRING);\n",
            "  optional int64 totalQuantity;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:19:07 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 05:19:08 INFO FileOutputCommitter: Saved output of task 'attempt_202508060519075738423377438526314_0007_m_000000_6' to file:/content/output/processed/java_order_revenue.parquet/_temporary/0/task_202508060519075738423377438526314_0007_m_000000\n",
            "25/08/06 05:19:08 INFO SparkHadoopMapRedUtil: attempt_202508060519075738423377438526314_0007_m_000000_6: Committed. Elapsed time: 2 ms.\n",
            "25/08/06 05:19:08 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 8138 bytes result sent to driver\n",
            "25/08/06 05:19:08 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 588 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:19:08 INFO DAGScheduler: ResultStage 7 (parquet at OrderRevenueAnalysis.java:26) finished in 0.682 s\n",
            "25/08/06 05:19:08 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:19:08 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:19:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 05:19:08 INFO DAGScheduler: Job 6 finished: parquet at OrderRevenueAnalysis.java:26, took 0.725187 s\n",
            "25/08/06 05:19:08 INFO FileFormatWriter: Start to commit write Job 8b9eb30e-6f01-41fa-a894-0c57674b13f4.\n",
            "25/08/06 05:19:08 INFO FileFormatWriter: Write Job 8b9eb30e-6f01-41fa-a894-0c57674b13f4 committed. Elapsed time: 43 ms.\n",
            "25/08/06 05:19:08 INFO FileFormatWriter: Finished processing stats for write job 8b9eb30e-6f01-41fa-a894-0c57674b13f4.\n",
            "25/08/06 05:19:08 INFO BlockManagerInfo: Removed broadcast_10_piece0 on c3748e2df8f0:40503 in memory (size: 23.2 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:19:08 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:19:08 INFO BlockManagerInfo: Removed broadcast_11_piece0 on c3748e2df8f0:40503 in memory (size: 92.5 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:19:08 INFO SparkUI: Stopped Spark web UI at http://c3748e2df8f0:4041\n",
            "25/08/06 05:19:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:19:08 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:19:08 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:19:08 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:19:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:19:08 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:19:08 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:19:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-b5438667-4783-42d4-87c1-4f87dc1ade3a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/output/processed/java_order_revenue.parquet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txDkS6X5z8-c",
        "outputId": "e0294ffb-657e-4e7c-ccef-3b0b35f216ac"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-8651b3cf-94aa-4aa7-ac73-91117b43d825-c000.snappy.parquet  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#View Parquet Output with PySpark\n",
        "df = spark.read.parquet(\"/content/output/processed/java_order_revenue.parquet\")\n",
        "df.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTjo3FJg0n-o",
        "outputId": "23665aec-b5c4-488c-88f2-47d7dbacd9d1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+-------------+\n",
            "|productName                        |totalQuantity|\n",
            "+-----------------------------------+-------------+\n",
            "|1996 Moto Guzzi 1100i              |999          |\n",
            "|1936 Chrysler Airflow              |983          |\n",
            "|18th Century Vintage Horse Carriage|907          |\n",
            "|The Titanic                        |952          |\n",
            "|1958 Setra Bus                     |972          |\n",
            "|Diamond T620 Semi-Skirted Tanker   |979          |\n",
            "|2001 Ferrari Enzo                  |1019         |\n",
            "|The Queen Mary                     |896          |\n",
            "|1930 Buick Marquette Phaeton       |1074         |\n",
            "|The Mayflower                      |898          |\n",
            "+-----------------------------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/src/java\n",
        "!mkdir -p /content/build/java\n",
        "!mkdir -p /content/output/processed\n"
      ],
      "metadata": {
        "id": "Yldi-9ee0wVg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/src/java/CustomerSalesAnalysis.java\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "public class CustomerSalesAnalysis {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"CustomerSalesAnalysis\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Load data\n",
        "        Dataset<Row> orders = spark.read().parquet(\"/content/data/parquet/orders.parquet\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"/content/data/parquet/payments.parquet\");\n",
        "        Dataset<Row> customers = spark.read().parquet(\"/content/data/parquet/customers.parquet\");\n",
        "\n",
        "        // Join orders -> payments -> customers\n",
        "        Dataset<Row> customerSales = orders.join(payments, \"customerNumber\")\n",
        "                                           .join(customers, \"customerNumber\")\n",
        "                                           .groupBy(col(\"customerName\"))\n",
        "                                           .agg(avg(\"amount\").alias(\"avgOrderValue\"),\n",
        "                                                sum(\"amount\").alias(\"totalSpent\"))\n",
        "                                           .orderBy(col(\"totalSpent\").desc());\n",
        "\n",
        "        // Save output\n",
        "        customerSales.write().mode(\"overwrite\").parquet(\"/content/output/processed/java_customer_sales.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj2sAB0K1I0g",
        "outputId": "baeeac3b-80ab-42af-edcd-fe65ce9ba664"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/src/java/CustomerSalesAnalysis.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/src/java/EmployeePerformanceAnalysis.java\n",
        "import org.apache.spark.sql.SparkSession;\n",
        "import org.apache.spark.sql.Dataset;\n",
        "import org.apache.spark.sql.Row;\n",
        "import static org.apache.spark.sql.functions.*;\n",
        "\n",
        "public class EmployeePerformanceAnalysis {\n",
        "    public static void main(String[] args) {\n",
        "        SparkSession spark = SparkSession.builder()\n",
        "                .appName(\"EmployeePerformanceAnalysis\")\n",
        "                .getOrCreate();\n",
        "\n",
        "        // Load data\n",
        "        Dataset<Row> employees = spark.read().parquet(\"/content/data/parquet/employees.parquet\");\n",
        "        Dataset<Row> customers = spark.read().parquet(\"/content/data/parquet/customers.parquet\");\n",
        "        Dataset<Row> payments = spark.read().parquet(\"/content/data/parquet/payments.parquet\");\n",
        "\n",
        "        // Join employees -> customers -> payments\n",
        "        Dataset<Row> empSales = employees.join(customers, \"employeeNumber\")\n",
        "                                         .join(payments, \"customerNumber\")\n",
        "                                         .groupBy(col(\"lastName\"), col(\"firstName\"))\n",
        "                                         .agg(sum(\"amount\").alias(\"totalSales\"),\n",
        "                                              countDistinct(\"customerNumber\").alias(\"uniqueCustomers\"))\n",
        "                                         .orderBy(col(\"totalSales\").desc());\n",
        "\n",
        "        // Save output\n",
        "        empSales.write().mode(\"overwrite\").parquet(\"/content/output/processed/java_employee_sales.parquet\");\n",
        "\n",
        "        spark.stop();\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ng4hHxB1KvC",
        "outputId": "1d05385f-da88-4651-cf11-7cc21703b8c7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/src/java/EmployeePerformanceAnalysis.java\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "CLASS_PATH=$(echo /usr/local/spark/jars/*.jar | tr ' ' ':')\n",
        "javac -cp \"$CLASS_PATH\" -d /content/build/java /content/src/java/*.java\n"
      ],
      "metadata": {
        "id": "gC9iNvy-1Nz2"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "CLASS_PATH=$(echo /usr/local/spark/jars/*.jar | tr ' ' ':')\n",
        "java -cp \"/content/build/java:$CLASS_PATH\" OrderRevenueAnalysis\n",
        "java -cp \"/content/build/java:$CLASS_PATH\" CustomerSalesAnalysis\n",
        "java -cp \"/content/build/java:$CLASS_PATH\" EmployeePerformanceAnalysis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "_CIvx1Ob1Uaa",
        "outputId": "21b0526c-4c52-496f-a477-4be0c5fa7fab"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet written to /content/output/processed/java_order_revenue.parquet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:23:24 INFO SparkContext: Running Spark version 3.5.0\n",
            "25/08/06 05:23:24 INFO SparkContext: OS info Linux, 6.1.123+, amd64\n",
            "25/08/06 05:23:24 INFO SparkContext: Java version 11.0.28\n",
            "25/08/06 05:23:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:23:24 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:23:24 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/06 05:23:24 INFO ResourceUtils: ==============================================================\n",
            "25/08/06 05:23:24 INFO SparkContext: Submitted application: OrderRevenueAnalysis\n",
            "25/08/06 05:23:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/06 05:23:24 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/06 05:23:24 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/06 05:23:25 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/06 05:23:25 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/06 05:23:25 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/06 05:23:25 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/06 05:23:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/06 05:23:25 INFO Utils: Successfully started service 'sparkDriver' on port 38987.\n",
            "25/08/06 05:23:25 INFO SparkEnv: Registering MapOutputTracker\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.5.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "25/08/06 05:23:25 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/06 05:23:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/06 05:23:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/06 05:23:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/06 05:23:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-83024428-5e64-4a25-bb4b-23540ab40c28\n",
            "25/08/06 05:23:25 INFO MemoryStore: MemoryStore started with capacity 1767.6 MiB\n",
            "25/08/06 05:23:25 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/06 05:23:26 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/06 05:23:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/06 05:23:26 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/06 05:23:26 INFO Executor: Starting executor ID driver on host c3748e2df8f0\n",
            "25/08/06 05:23:26 INFO Executor: OS info Linux, 6.1.123+, amd64\n",
            "25/08/06 05:23:26 INFO Executor: Java version 11.0.28\n",
            "25/08/06 05:23:26 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/06 05:23:26 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@797c3c3b for default.\n",
            "25/08/06 05:23:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42323.\n",
            "25/08/06 05:23:26 INFO NettyBlockTransferService: Server created on c3748e2df8f0:42323\n",
            "25/08/06 05:23:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/06 05:23:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c3748e2df8f0, 42323, None)\n",
            "25/08/06 05:23:26 INFO BlockManagerMasterEndpoint: Registering block manager c3748e2df8f0:42323 with 1767.6 MiB RAM, BlockManagerId(driver, c3748e2df8f0, 42323, None)\n",
            "25/08/06 05:23:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c3748e2df8f0, 42323, None)\n",
            "25/08/06 05:23:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, c3748e2df8f0, 42323, None)\n",
            "25/08/06 05:23:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/06 05:23:27 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/06 05:23:29 INFO InMemoryFileIndex: It took 84 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:23:29 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:14\n",
            "25/08/06 05:23:29 INFO DAGScheduler: Got job 0 (parquet at OrderRevenueAnalysis.java:14) with 1 output partitions\n",
            "25/08/06 05:23:29 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at OrderRevenueAnalysis.java:14)\n",
            "25/08/06 05:23:29 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:23:29 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:23:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at OrderRevenueAnalysis.java:14), which has no missing parents\n",
            "25/08/06 05:23:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.8 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:23:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:23:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on c3748e2df8f0:42323 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:23:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:23:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at OrderRevenueAnalysis.java:14) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:23:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:23:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 7751 bytes) \n",
            "25/08/06 05:23:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/06 05:23:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1942 bytes result sent to driver\n",
            "25/08/06 05:23:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 889 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:23:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:23:31 INFO DAGScheduler: ResultStage 0 (parquet at OrderRevenueAnalysis.java:14) finished in 1.213 s\n",
            "25/08/06 05:23:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:23:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/06 05:23:31 INFO DAGScheduler: Job 0 finished: parquet at OrderRevenueAnalysis.java:14, took 1.332020 s\n",
            "25/08/06 05:23:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on c3748e2df8f0:42323 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:23:32 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:23:32 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:15\n",
            "25/08/06 05:23:32 INFO DAGScheduler: Got job 1 (parquet at OrderRevenueAnalysis.java:15) with 1 output partitions\n",
            "25/08/06 05:23:32 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at OrderRevenueAnalysis.java:15)\n",
            "25/08/06 05:23:32 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:23:32 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:23:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at OrderRevenueAnalysis.java:15), which has no missing parents\n",
            "25/08/06 05:23:33 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 102.8 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:23:33 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.5 MiB)\n",
            "25/08/06 05:23:33 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on c3748e2df8f0:42323 (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:23:33 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at OrderRevenueAnalysis.java:15) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:23:33 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:23:33 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 7757 bytes) \n",
            "25/08/06 05:23:33 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/06 05:23:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1899 bytes result sent to driver\n",
            "25/08/06 05:23:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 78 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:23:33 INFO DAGScheduler: ResultStage 1 (parquet at OrderRevenueAnalysis.java:15) finished in 0.148 s\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:23:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:23:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Job 1 finished: parquet at OrderRevenueAnalysis.java:15, took 0.180194 s\n",
            "25/08/06 05:23:33 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
            "25/08/06 05:23:33 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:16\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Got job 2 (parquet at OrderRevenueAnalysis.java:16) with 1 output partitions\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at OrderRevenueAnalysis.java:16)\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at parquet at OrderRevenueAnalysis.java:16), which has no missing parents\n",
            "25/08/06 05:23:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.8 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:23:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 1767.3 MiB)\n",
            "25/08/06 05:23:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on c3748e2df8f0:42323 (size: 36.9 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at parquet at OrderRevenueAnalysis.java:16) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:23:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:23:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 7753 bytes) \n",
            "25/08/06 05:23:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
            "25/08/06 05:23:33 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2021 bytes result sent to driver\n",
            "25/08/06 05:23:33 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 85 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:23:33 INFO DAGScheduler: ResultStage 2 (parquet at OrderRevenueAnalysis.java:16) finished in 0.155 s\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:23:33 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:23:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/06 05:23:33 INFO DAGScheduler: Job 2 finished: parquet at OrderRevenueAnalysis.java:16, took 0.173220 s\n",
            "25/08/06 05:23:35 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber),IsNotNull(productCode)\n",
            "25/08/06 05:23:35 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#14L),isnotnull(productCode#15)\n",
            "25/08/06 05:23:35 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber)\n",
            "25/08/06 05:23:35 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#0L)\n",
            "25/08/06 05:23:35 INFO FileSourceStrategy: Pushed Filters: IsNotNull(productCode)\n",
            "25/08/06 05:23:35 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(productCode#24)\n",
            "25/08/06 05:23:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on c3748e2df8f0:42323 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:23:36 INFO BlockManagerInfo: Removed broadcast_2_piece0 on c3748e2df8f0:42323 in memory (size: 36.9 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:23:36 INFO CodeGenerator: Code generated in 438.565014 ms\n",
            "25/08/06 05:23:36 INFO CodeGenerator: Code generated in 444.149679 ms\n",
            "25/08/06 05:23:36 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 200.0 KiB, free 1767.4 MiB)\n",
            "25/08/06 05:23:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.2 KiB, free 1767.2 MiB)\n",
            "25/08/06 05:23:36 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1767.2 MiB)\n",
            "25/08/06 05:23:36 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on c3748e2df8f0:42323 (size: 34.6 KiB, free: 1767.6 MiB)\n",
            "25/08/06 05:23:36 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:23:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:23:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on c3748e2df8f0:42323 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:36 INFO SparkContext: Created broadcast 4 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:23:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:23:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:23:37 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:23:37 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Final stage: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:23:37 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.0 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:23:37 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:23:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on c3748e2df8f0:42323 (size: 6.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:37 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:23:37 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:23:37 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 8223 bytes) \n",
            "25/08/06 05:23:37 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
            "25/08/06 05:23:37 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.3 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:23:37 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 1767.1 MiB)\n",
            "25/08/06 05:23:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on c3748e2df8f0:42323 (size: 6.3 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:37 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:23:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[11] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:23:37 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:23:37 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 8221 bytes) \n",
            "25/08/06 05:23:37 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
            "25/08/06 05:23:37 INFO CodeGenerator: Code generated in 50.672488 ms\n",
            "25/08/06 05:23:37 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orders.parquet, range: 0-14618, partition values: [empty row]\n",
            "25/08/06 05:23:37 INFO CodeGenerator: Code generated in 72.072293 ms\n",
            "25/08/06 05:23:37 INFO FileScanRDD: Reading File path: file:///content/data/parquet/products.parquet, range: 0-20163, partition values: [empty row]\n",
            "25/08/06 05:23:37 INFO FilterCompat: Filtering using predicate: noteq(productCode, null)\n",
            "25/08/06 05:23:37 INFO FilterCompat: Filtering using predicate: noteq(orderNumber, null)\n",
            "25/08/06 05:23:37 INFO ColumnIndexFilter: No offset index for column productCode is available; Unable to do filtering\n",
            "25/08/06 05:23:37 INFO ColumnIndexFilter: No offset index for column orderNumber is available; Unable to do filtering\n",
            "25/08/06 05:23:37 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 05:23:37 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
            "25/08/06 05:23:38 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 5462 bytes result sent to driver\n",
            "25/08/06 05:23:38 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 3502 bytes result sent to driver\n",
            "25/08/06 05:23:38 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 839 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:23:38 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:23:38 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 899 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:23:38 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:23:38 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.857 s\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:23:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.946770 s\n",
            "25/08/06 05:23:38 INFO DAGScheduler: ResultStage 3 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.928 s\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:23:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.955765 s\n",
            "25/08/06 05:23:38 INFO CodeGenerator: Code generated in 30.174783 ms\n",
            "25/08/06 05:23:38 INFO CodeGenerator: Code generated in 25.734337 ms\n",
            "25/08/06 05:23:38 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 1026.6 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:23:38 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.4 KiB, free 1766.1 MiB)\n",
            "25/08/06 05:23:38 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on c3748e2df8f0:42323 (size: 3.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:38 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:23:38 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 MiB, free 1734.1 MiB)\n",
            "25/08/06 05:23:38 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 1734.1 MiB)\n",
            "25/08/06 05:23:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on c3748e2df8f0:42323 (size: 4.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:38 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
            "25/08/06 05:23:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber),IsNotNull(productCode)\n",
            "25/08/06 05:23:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#14L),isnotnull(productCode#15)\n",
            "25/08/06 05:23:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(orderNumber),IsNotNull(productCode)\n",
            "25/08/06 05:23:38 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(orderNumber#14L),isnotnull(productCode#15)\n",
            "25/08/06 05:23:38 INFO CodeGenerator: Code generated in 171.724717 ms\n",
            "25/08/06 05:23:38 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.3 KiB, free 1733.9 MiB)\n",
            "25/08/06 05:23:38 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1733.9 MiB)\n",
            "25/08/06 05:23:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on c3748e2df8f0:42323 (size: 34.7 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:38 INFO SparkContext: Created broadcast 9 from parquet at OrderRevenueAnalysis.java:26\n",
            "25/08/06 05:23:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Registering RDD 17 (parquet at OrderRevenueAnalysis.java:26) as input to shuffle 0\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Got map stage job 5 (parquet at OrderRevenueAnalysis.java:26) with 1 output partitions\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (parquet at OrderRevenueAnalysis.java:26)\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[17] at parquet at OrderRevenueAnalysis.java:26), which has no missing parents\n",
            "25/08/06 05:23:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 54.1 KiB, free 1733.8 MiB)\n",
            "25/08/06 05:23:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 23.2 KiB, free 1733.8 MiB)\n",
            "25/08/06 05:23:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on c3748e2df8f0:42323 (size: 23.2 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:38 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:23:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[17] at parquet at OrderRevenueAnalysis.java:26) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:23:38 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:23:38 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (c3748e2df8f0, executor driver, partition 0, PROCESS_LOCAL, 8216 bytes) \n",
            "25/08/06 05:23:38 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
            "25/08/06 05:23:39 INFO CodeGenerator: Code generated in 132.763442 ms\n",
            "25/08/06 05:23:39 INFO CodeGenerator: Code generated in 36.628147 ms\n",
            "25/08/06 05:23:39 INFO CodeGenerator: Code generated in 20.887355 ms\n",
            "25/08/06 05:23:39 INFO CodeGenerator: Code generated in 20.243501 ms\n",
            "25/08/06 05:23:39 INFO CodeGenerator: Code generated in 16.099735 ms\n",
            "25/08/06 05:23:39 INFO FileScanRDD: Reading File path: file:///content/data/parquet/orderdetails.parquet, range: 0-26383, partition values: [empty row]\n",
            "25/08/06 05:23:39 INFO FilterCompat: Filtering using predicate: and(noteq(orderNumber, null), noteq(productCode, null))\n",
            "25/08/06 05:23:39 INFO ColumnIndexFilter: No offset index for column orderNumber is available; Unable to do filtering\n",
            "25/08/06 05:23:39 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4766 bytes result sent to driver\n",
            "25/08/06 05:23:39 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 576 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:23:39 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:23:39 INFO DAGScheduler: ShuffleMapStage 5 (parquet at OrderRevenueAnalysis.java:26) finished in 0.631 s\n",
            "25/08/06 05:23:39 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/06 05:23:39 INFO DAGScheduler: running: Set()\n",
            "25/08/06 05:23:39 INFO DAGScheduler: waiting: Set()\n",
            "25/08/06 05:23:39 INFO DAGScheduler: failed: Set()\n",
            "25/08/06 05:23:39 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/06 05:23:39 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:23:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:23:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:23:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:23:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:23:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:23:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:23:39 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/06 05:23:39 INFO CodeGenerator: Code generated in 47.94735 ms\n",
            "25/08/06 05:23:39 INFO BlockManagerInfo: Removed broadcast_5_piece0 on c3748e2df8f0:42323 in memory (size: 6.4 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:39 INFO SparkContext: Starting job: parquet at OrderRevenueAnalysis.java:26\n",
            "25/08/06 05:23:39 INFO DAGScheduler: Got job 6 (parquet at OrderRevenueAnalysis.java:26) with 1 output partitions\n",
            "25/08/06 05:23:39 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at OrderRevenueAnalysis.java:26)\n",
            "25/08/06 05:23:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "25/08/06 05:23:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/06 05:23:39 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[20] at parquet at OrderRevenueAnalysis.java:26), which has no missing parents\n",
            "25/08/06 05:23:39 INFO BlockManagerInfo: Removed broadcast_6_piece0 on c3748e2df8f0:42323 in memory (size: 6.3 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:39 INFO BlockManagerInfo: Removed broadcast_10_piece0 on c3748e2df8f0:42323 in memory (size: 23.2 KiB, free: 1767.5 MiB)\n",
            "25/08/06 05:23:39 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 250.1 KiB, free 1733.7 MiB)\n",
            "25/08/06 05:23:39 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 92.5 KiB, free 1733.6 MiB)\n",
            "25/08/06 05:23:39 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on c3748e2df8f0:42323 (size: 92.5 KiB, free: 1767.4 MiB)\n",
            "25/08/06 05:23:39 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
            "25/08/06 05:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[20] at parquet at OrderRevenueAnalysis.java:26) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/06 05:23:39 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/06 05:23:39 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (c3748e2df8f0, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
            "25/08/06 05:23:39 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)\n",
            "25/08/06 05:23:39 INFO ShuffleBlockFetcherIterator: Getting 1 (8.8 KiB) non-empty blocks including 1 (8.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/06 05:23:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms\n",
            "25/08/06 05:23:40 INFO CodeGenerator: Code generated in 24.256394 ms\n",
            "25/08/06 05:23:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:23:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:23:40 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:23:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "25/08/06 05:23:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "25/08/06 05:23:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
            "25/08/06 05:23:40 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:23:40 INFO CodecConfig: Compression: SNAPPY\n",
            "25/08/06 05:23:40 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n",
            "25/08/06 05:23:40 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
            "{\n",
            "  \"type\" : \"struct\",\n",
            "  \"fields\" : [ {\n",
            "    \"name\" : \"productName\",\n",
            "    \"type\" : \"string\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  }, {\n",
            "    \"name\" : \"totalQuantity\",\n",
            "    \"type\" : \"long\",\n",
            "    \"nullable\" : true,\n",
            "    \"metadata\" : { }\n",
            "  } ]\n",
            "}\n",
            "and corresponding Parquet message type:\n",
            "message spark_schema {\n",
            "  optional binary productName (STRING);\n",
            "  optional int64 totalQuantity;\n",
            "}\n",
            "\n",
            "       \n",
            "25/08/06 05:23:40 INFO CodecPool: Got brand-new compressor [.snappy]\n",
            "25/08/06 05:23:40 INFO FileOutputCommitter: Saved output of task 'attempt_202508060523396031730270919862605_0007_m_000000_6' to file:/content/output/processed/java_order_revenue.parquet/_temporary/0/task_202508060523396031730270919862605_0007_m_000000\n",
            "25/08/06 05:23:40 INFO SparkHadoopMapRedUtil: attempt_202508060523396031730270919862605_0007_m_000000_6: Committed. Elapsed time: 1 ms.\n",
            "25/08/06 05:23:40 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 8138 bytes result sent to driver\n",
            "25/08/06 05:23:40 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 491 ms on c3748e2df8f0 (executor driver) (1/1)\n",
            "25/08/06 05:23:40 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/06 05:23:40 INFO DAGScheduler: ResultStage 7 (parquet at OrderRevenueAnalysis.java:26) finished in 0.567 s\n",
            "25/08/06 05:23:40 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/06 05:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/06 05:23:40 INFO DAGScheduler: Job 6 finished: parquet at OrderRevenueAnalysis.java:26, took 0.615886 s\n",
            "25/08/06 05:23:40 INFO FileFormatWriter: Start to commit write Job 1d12b2ff-6fda-425d-8724-53ef0d5d874b.\n",
            "25/08/06 05:23:40 INFO FileFormatWriter: Write Job 1d12b2ff-6fda-425d-8724-53ef0d5d874b committed. Elapsed time: 44 ms.\n",
            "25/08/06 05:23:40 INFO FileFormatWriter: Finished processing stats for write job 1d12b2ff-6fda-425d-8724-53ef0d5d874b.\n",
            "25/08/06 05:23:40 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:23:40 INFO SparkUI: Stopped Spark web UI at http://c3748e2df8f0:4041\n",
            "25/08/06 05:23:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/08/06 05:23:40 INFO MemoryStore: MemoryStore cleared\n",
            "25/08/06 05:23:40 INFO BlockManager: BlockManager stopped\n",
            "25/08/06 05:23:40 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/08/06 05:23:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/08/06 05:23:40 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/08/06 05:23:40 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/08/06 05:23:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-450af386-ee7f-423e-ba4d-0157f808af97\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:23:43 INFO SparkContext: Running Spark version 3.5.0\n",
            "25/08/06 05:23:43 INFO SparkContext: OS info Linux, 6.1.123+, amd64\n",
            "25/08/06 05:23:43 INFO SparkContext: Java version 11.0.28\n",
            "25/08/06 05:23:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:23:44 ERROR SparkContext: Error initializing SparkContext.\n",
            "org.apache.spark.SparkException: A master URL must be set in your configuration\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:414)\n",
            "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)\n",
            "\tat CustomerSalesAnalysis.main(CustomerSalesAnalysis.java:10)\n",
            "25/08/06 05:23:44 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:23:44 INFO SparkContext: Successfully stopped SparkContext\n",
            "Exception in thread \"main\" org.apache.spark.SparkException: A master URL must be set in your configuration\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:414)\n",
            "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)\n",
            "\tat CustomerSalesAnalysis.main(CustomerSalesAnalysis.java:10)\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
            "25/08/06 05:23:52 INFO SparkContext: Running Spark version 3.5.0\n",
            "25/08/06 05:23:52 INFO SparkContext: OS info Linux, 6.1.123+, amd64\n",
            "25/08/06 05:23:52 INFO SparkContext: Java version 11.0.28\n",
            "25/08/06 05:23:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/06 05:23:54 ERROR SparkContext: Error initializing SparkContext.\n",
            "org.apache.spark.SparkException: A master URL must be set in your configuration\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:414)\n",
            "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)\n",
            "\tat EmployeePerformanceAnalysis.main(EmployeePerformanceAnalysis.java:10)\n",
            "25/08/06 05:23:54 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/08/06 05:23:54 INFO SparkContext: Successfully stopped SparkContext\n",
            "Exception in thread \"main\" org.apache.spark.SparkException: A master URL must be set in your configuration\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:414)\n",
            "\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)\n",
            "\tat scala.Option.getOrElse(Option.scala:189)\n",
            "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)\n",
            "\tat EmployeePerformanceAnalysis.main(EmployeePerformanceAnalysis.java:10)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'b'CLASS_PATH=$(echo /usr/local/spark/jars/*.jar | tr \\' \\' \\':\\')\\njava -cp \"/content/build/java:$CLASS_PATH\" OrderRevenueAnalysis\\njava -cp \"/content/build/java:$CLASS_PATH\" CustomerSalesAnalysis\\njava -cp \"/content/build/java:$CLASS_PATH\" EmployeePerformanceAnalysis\\n'' returned non-zero exit status 1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2757033434.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLASS_PATH=$(echo /usr/local/spark/jars/*.jar | tr \\' \\' \\':\\')\\njava -cp \"/content/build/java:$CLASS_PATH\" OrderRevenueAnalysis\\njava -cp \"/content/build/java:$CLASS_PATH\" CustomerSalesAnalysis\\njava -cp \"/content/build/java:$CLASS_PATH\" EmployeePerformanceAnalysis\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'CLASS_PATH=$(echo /usr/local/spark/jars/*.jar | tr \\' \\' \\':\\')\\njava -cp \"/content/build/java:$CLASS_PATH\" OrderRevenueAnalysis\\njava -cp \"/content/build/java:$CLASS_PATH\" CustomerSalesAnalysis\\njava -cp \"/content/build/java:$CLASS_PATH\" EmployeePerformanceAnalysis\\n'' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Product Demand Summary Analysis\n",
        "# Output: product_demand_summary.parquet\n",
        "\n",
        "# 1. Join orderdetails and products\n",
        "product_demand = orderdetails.join(products, \"productCode\")\n",
        "\n",
        "# 2. Group by productCode and productName\n",
        "product_demand_summary = product_demand.groupBy(\"productCode\", \"productName\") \\\n",
        "    .agg(\n",
        "        F.countDistinct(\"orderNumber\").alias(\"orderCount\"),\n",
        "        F.sum(\"quantityOrdered\").alias(\"totalOrderedQty\"),\n",
        "        F.avg(\"priceEach\").alias(\"averagePriceEach\")\n",
        "    )\n",
        "\n",
        "# 3. Save to Parquet\n",
        "output_path = \"/content/output/processed/product_demand_summary.parquet\"\n",
        "product_demand_summary.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(\"✅ Product Demand Summary saved to:\", output_path)\n",
        "\n",
        "# 4. Show top 10 products by quantity\n",
        "display(product_demand_summary.orderBy(F.desc(\"totalOrderedQty\")).limit(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MI9aJza31X5q",
        "outputId": "75850553-31a1-4473-b076-17bfb5f126a2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Product Demand Summary saved to: /content/output/processed/product_demand_summary.parquet\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[productCode: string, productName: string, orderCount: bigint, totalOrderedQty: bigint, averagePriceEach: double]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_parquet(\"/content/output/processed/product_demand_summary.parquet\").head(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "collapsed": true,
        "id": "YRr3uHjh1r3a",
        "outputId": "d0ee57de-8aa3-4ab2-c8ad-51ca23f6a8d1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  productCode                   productName  orderCount  totalOrderedQty  \\\n",
              "0    S12_2823              2002 Suzuki XREO          28             1028   \n",
              "1    S18_1889   1948 Porsche 356-A Roadster          27              972   \n",
              "2    S24_1578             1997 BMW R 1100 S          28             1033   \n",
              "3    S24_3371     1971 Alpine Renault 1600s          27              969   \n",
              "4    S24_2972       1982 Lamborghini Diablo          27              912   \n",
              "5    S18_3140             1903 Ford Model A          27              883   \n",
              "6   S700_2824               1982 Camaro Z28          28              997   \n",
              "7    S12_4473             1957 Chevy Pickup          28             1056   \n",
              "8    S18_2870  1999 Indy 500 Monte Carlo SS          25              855   \n",
              "9    S32_1268   1980’s GM Manhattan Express          28              911   \n",
              "\n",
              "   averagePriceEach  \n",
              "0        132.168929  \n",
              "1         68.530000  \n",
              "2        101.953571  \n",
              "3         54.132593  \n",
              "4         33.942593  \n",
              "5        125.714074  \n",
              "6         89.408929  \n",
              "7        103.902143  \n",
              "8        118.377600  \n",
              "9         85.853214  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26ecb527-5776-4642-a7c0-7a863a8006e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>productCode</th>\n",
              "      <th>productName</th>\n",
              "      <th>orderCount</th>\n",
              "      <th>totalOrderedQty</th>\n",
              "      <th>averagePriceEach</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>S12_2823</td>\n",
              "      <td>2002 Suzuki XREO</td>\n",
              "      <td>28</td>\n",
              "      <td>1028</td>\n",
              "      <td>132.168929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>S18_1889</td>\n",
              "      <td>1948 Porsche 356-A Roadster</td>\n",
              "      <td>27</td>\n",
              "      <td>972</td>\n",
              "      <td>68.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>S24_1578</td>\n",
              "      <td>1997 BMW R 1100 S</td>\n",
              "      <td>28</td>\n",
              "      <td>1033</td>\n",
              "      <td>101.953571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>S24_3371</td>\n",
              "      <td>1971 Alpine Renault 1600s</td>\n",
              "      <td>27</td>\n",
              "      <td>969</td>\n",
              "      <td>54.132593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>S24_2972</td>\n",
              "      <td>1982 Lamborghini Diablo</td>\n",
              "      <td>27</td>\n",
              "      <td>912</td>\n",
              "      <td>33.942593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>S18_3140</td>\n",
              "      <td>1903 Ford Model A</td>\n",
              "      <td>27</td>\n",
              "      <td>883</td>\n",
              "      <td>125.714074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>S700_2824</td>\n",
              "      <td>1982 Camaro Z28</td>\n",
              "      <td>28</td>\n",
              "      <td>997</td>\n",
              "      <td>89.408929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>S12_4473</td>\n",
              "      <td>1957 Chevy Pickup</td>\n",
              "      <td>28</td>\n",
              "      <td>1056</td>\n",
              "      <td>103.902143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>S18_2870</td>\n",
              "      <td>1999 Indy 500 Monte Carlo SS</td>\n",
              "      <td>25</td>\n",
              "      <td>855</td>\n",
              "      <td>118.377600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>S32_1268</td>\n",
              "      <td>1980’s GM Manhattan Express</td>\n",
              "      <td>28</td>\n",
              "      <td>911</td>\n",
              "      <td>85.853214</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26ecb527-5776-4642-a7c0-7a863a8006e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-26ecb527-5776-4642-a7c0-7a863a8006e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-26ecb527-5776-4642-a7c0-7a863a8006e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-dd22e8c3-6839-4bb1-a6aa-dc37e3ecaf22\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd22e8c3-6839-4bb1-a6aa-dc37e3ecaf22')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-dd22e8c3-6839-4bb1-a6aa-dc37e3ecaf22 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"productCode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"S18_2870\",\n          \"S18_1889\",\n          \"S18_3140\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"productName\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"1999 Indy 500 Monte Carlo SS\",\n          \"1948 Porsche 356-A Roadster\",\n          \"1903 Ford Model A\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"orderCount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 25,\n        \"max\": 28,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          28,\n          27,\n          25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"totalOrderedQty\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 68,\n        \"min\": 855,\n        \"max\": 1056,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          855,\n          972,\n          883\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"averagePriceEach\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31.768284245974563,\n        \"min\": 33.9425925925926,\n        \"max\": 132.16892857142855,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          118.37760000000002,\n          68.53,\n          125.71407407407405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "np7DXlu02MnE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}