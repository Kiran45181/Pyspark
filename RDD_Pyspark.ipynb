{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb9VebnrY29SvmWGKF9c35",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kiran45181/Pyspark/blob/main/RDD_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RDD - Resilient Distributed Dataset"
      ],
      "metadata": {
        "id": "eQ5MU2Z8mDkp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hxi65ITml-2Q",
        "outputId": "ee3024b4-4c6e-4b5b-a865-29ed893f890c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mapped RDD: [2, 4, 6, 8, 10]\n",
            "Filtered RDD: [2, 4]\n",
            "Sum using reduce: 15\n",
            "Flatmap RDD: [1, 2, 2, 3, 3, 4, 4, 5, 5, 6]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Flatmap String RDD: ['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd']\n",
            "odd [1, 3, 5]\n",
            "even [2, 4]\n",
            "[('odd', [1, 3, 5]), ('even', [2, 4])]\n",
            "Count of elements: 5\n",
            "First element: 1\n"
          ]
        }
      ],
      "source": [
        "# =====================================\n",
        "# 1. Install and Import PySpark\n",
        "# =====================================\n",
        "# Uncomment this line in Colab (skip if PySpark already installed)\n",
        "# !pip install pyspark\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# =====================================\n",
        "# 2. Stop Previous Spark Context (Fix Error)\n",
        "# =====================================\n",
        "try:\n",
        "    sc.stop()\n",
        "except:\n",
        "    pass  # No active SparkContext to stop\n",
        "\n",
        "# =====================================\n",
        "# 3. Initialize SparkConf and SparkContext\n",
        "# =====================================\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"RDD Example\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# =====================================\n",
        "# 4. Create RDD\n",
        "# =====================================\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "\n",
        "# =====================================\n",
        "# 5. Transformation: Map\n",
        "# Multiply each element by 2\n",
        "# =====================================\n",
        "rdd_mapped = rdd.map(lambda x: x * 2)\n",
        "\n",
        "# Action: Collect\n",
        "print(\"Mapped RDD:\", rdd_mapped.collect())   # [2, 4, 6, 8, 10]\n",
        "\n",
        "# =====================================\n",
        "# 6. Transformation: Filter\n",
        "# Keep only even numbers\n",
        "# =====================================\n",
        "rdd_filtered = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Action: Collect\n",
        "print(\"Filtered RDD:\", rdd_filtered.collect())  # [2, 4]\n",
        "\n",
        "# =====================================\n",
        "# 7. Action: Reduce\n",
        "# Sum of all elements\n",
        "# =====================================\n",
        "sum_result = rdd.reduce(lambda a, b: a + b)\n",
        "print(\"Sum using reduce:\", sum_result)  # 15\n",
        "\n",
        "\n",
        "\n",
        "#Transformation : flatmap\n",
        "rdd_flatmap = rdd.flatMap(lambda x: range(x,x+2))\n",
        "print(\"Flatmap RDD:\", rdd_flatmap.collect())\n",
        "\n",
        "\n",
        "rdd_nested_list = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "rdd_flatmap_nested = rdd_nested_list.flatMap(lambda x: x)\n",
        "print(rdd_flatmap_nested.collect())\n",
        "\n",
        "\n",
        "rdd_flatmap_string = sc.parallelize([\"hello\", \"world\"])\n",
        "rdd_flatmap_string_result = rdd_flatmap_string.flatMap(lambda x: list(x))\n",
        "print(\"Flatmap String RDD:\", rdd_flatmap_string_result.collect())\n",
        "\n",
        "\n",
        "\n",
        "#Input : 1,2,3,4,5\n",
        "rdd_group = sc.parallelize([1,2,3,4,5])\n",
        "#use group by on above series like odd and even\n",
        "rdd_grouped = rdd_group.groupBy(lambda x: \"even\" if x % 2 == 0 else \"odd\")\n",
        "\n",
        "#print like key value pair\n",
        "# for key, value in rdd_grouped.collect():\n",
        "#     print(key, list(value))\n",
        "\n",
        "\n",
        "print([(key,list(value)) for key , value in rdd_grouped.collect()])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 8. Action: Count\n",
        "# Total number of elements\n",
        "# =====================================\n",
        "count_result = rdd.count()\n",
        "print(\"Count of elements:\", count_result)  # 5\n",
        "\n",
        "# =====================================\n",
        "# 9. Action: First\n",
        "# Get first element\n",
        "# =====================================\n",
        "first_element = rdd.first()\n",
        "print(\"First element:\", first_element)  # 1\n",
        "\n",
        "# =====================================\n",
        "# 10. Stop Spark Context\n",
        "# =====================================\n",
        "sc.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DATAFRAME"
      ],
      "metadata": {
        "id": "dlK1LZG_7t2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"John\", \"HR\", 5000),\n",
        "    (2, \"Jane\", \"IT\", 8000),\n",
        "    (3, \"Mike\", \"IT\", 6000),\n",
        "    (4, \"Sara\", \"Finance\", 7000),\n",
        "    (5, \"David\", \"HR\", 5500)\n",
        "]\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
        "\n",
        "# Define column names\n",
        "columns = [\"ID\", \"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "# Create a DataFrame from the sample data\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "V_ZfF7b3yfGg",
        "outputId": "5ff19928-12b7-489b-b1bc-a4b9876e3833"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| ID| Name|Department|Salary|\n",
            "+---+-----+----------+------+\n",
            "|  1| John|        HR|  5000|\n",
            "|  2| Jane|        IT|  8000|\n",
            "|  3| Mike|        IT|  6000|\n",
            "|  4| Sara|   Finance|  7000|\n",
            "|  5|David|        HR|  5500|\n",
            "+---+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"Name\",\"Salary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k5Cbm-tf7QnI",
        "outputId": "be5b8d90-83f1-4df0-cc86-a12073491e1e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| Name|Salary|\n",
            "+-----+------+\n",
            "| John|  5000|\n",
            "| Jane|  8000|\n",
            "| Mike|  6000|\n",
            "| Sara|  7000|\n",
            "|David|  5500|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df.Salary > 6000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7LtMpHz375GN",
        "outputId": "60cae379-99ed-4a52-a2a6-c6378073b800"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----------+------+\n",
            "| ID|Name|Department|Salary|\n",
            "+---+----+----------+------+\n",
            "|  2|Jane|        IT|  8000|\n",
            "|  4|Sara|   Finance|  7000|\n",
            "+---+----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = df.withColumn(\"Bonus\", col(\"Salary\") * 0.1)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yaKYk4Ot8Cym",
        "outputId": "87c924dd-2b40-4b4d-f759-416f5b3de770"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+-----+\n",
            "| ID| Name|Department|Salary|Bonus|\n",
            "+---+-----+----------+------+-----+\n",
            "|  1| John|        HR|  5000|500.0|\n",
            "|  2| Jane|        IT|  8000|800.0|\n",
            "|  3| Mike|        IT|  6000|600.0|\n",
            "|  4| Sara|   Finance|  7000|700.0|\n",
            "|  5|David|        HR|  5500|550.0|\n",
            "+---+-----+----------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= df.drop(\"Bonus\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RyHPZB2A8WBN",
        "outputId": "1cee6cd9-e328-47fe-b6a3-eaf77b995b5f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+------+\n",
            "| ID| Name|Department|Salary|\n",
            "+---+-----+----------+------+\n",
            "|  1| John|        HR|  5000|\n",
            "|  2| Jane|        IT|  8000|\n",
            "|  3| Mike|        IT|  6000|\n",
            "|  4| Sara|   Finance|  7000|\n",
            "|  5|David|        HR|  5500|\n",
            "+---+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumnRenamed(\"Salary\" , \"Salary_After_Tax\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cEEcCN3j8fV_",
        "outputId": "2fca803d-a1c9-4870-8ed2-61f08a57acca"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+----------------+\n",
            "| ID| Name|Department|Salary_After_Tax|\n",
            "+---+-----+----------+----------------+\n",
            "|  1| John|        HR|            5000|\n",
            "|  2| Jane|        IT|            8000|\n",
            "|  3| Mike|        IT|            6000|\n",
            "|  4| Sara|   Finance|            7000|\n",
            "|  5|David|        HR|            5500|\n",
            "+---+-----+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "df.groupBy(\"Department\").agg(avg(\"Salary_After_Tax\")).alias(\"Average Salary\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PEs9kNzh8zcf",
        "outputId": "70f867bf-2ba0-4cff-8b9a-1c7bf4da1f34"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------------+\n",
            "|Department|avg(Salary_After_Tax)|\n",
            "+----------+---------------------+\n",
            "|        HR|               5250.0|\n",
            "|   Finance|               7000.0|\n",
            "|        IT|               7000.0|\n",
            "+----------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Department\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "L5Arqdu29F3S",
        "outputId": "1ffa0fa7-beb0-444f-dc0b-40dc030a3b5a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|Department|count|\n",
            "+----------+-----+\n",
            "|        HR|    2|\n",
            "|   Finance|    1|\n",
            "|        IT|    2|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort(df[\"Salary_After_Tax\"].desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kjhqt3n19S8x",
        "outputId": "3c7fa334-f13e-438f-d767-b6dc4e81b7d9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+----------------+\n",
            "| ID| Name|Department|Salary_After_Tax|\n",
            "+---+-----+----------+----------------+\n",
            "|  2| Jane|        IT|            8000|\n",
            "|  4| Sara|   Finance|            7000|\n",
            "|  3| Mike|        IT|            6000|\n",
            "|  5|David|        HR|            5500|\n",
            "|  1| John|        HR|            5000|\n",
            "+---+-----+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = df.collect()\n",
        "\n",
        "for row in data_list:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1sUT6cSG9ltH",
        "outputId": "41383de3-179c-409d-87ae-ed8aca6e286f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(ID=1, Name='John', Department='HR', Salary_After_Tax=5000)\n",
            "Row(ID=2, Name='Jane', Department='IT', Salary_After_Tax=8000)\n",
            "Row(ID=3, Name='Mike', Department='IT', Salary_After_Tax=6000)\n",
            "Row(ID=4, Name='Sara', Department='Finance', Salary_After_Tax=7000)\n",
            "Row(ID=5, Name='David', Department='HR', Salary_After_Tax=5500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DataSet"
      ],
      "metadata": {
        "id": "M-VlBqFY-s2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Get the existing SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "rdd = sc.parallelize([Row(name=\"RAJ\", age=23),Row(name=\"Hari\",age=21)])\n",
        "dataset=spark.createDataFrame(rdd)\n",
        "dataset.show()\n",
        "dataset.filter(col(\"age\") > 20).show()\n",
        "dataset.select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk3aVFeZ-Rs2",
        "outputId": "ab38fb03-5607-4a2a-b684-1600ffbb606a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "| RAJ| 23|\n",
            "|Hari| 21|\n",
            "+----+---+\n",
            "\n",
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "| RAJ| 23|\n",
            "|Hari| 21|\n",
            "+----+---+\n",
            "\n",
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "| RAJ|\n",
            "|Hari|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SparkSQLBasics\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", \"Sales\", 3000),\n",
        "    (2, \"Bob\", \"IT\", 4000),\n",
        "    (3, \"Cathy\", \"HR\", 3500),\n",
        "    (4, \"David\", \"Sales\", 4500),\n",
        "    (5, \"Eva\", \"IT\", 4200)\n",
        "]\n",
        "columns = [\"EmpID\", \"Name\", \"Department\", \"Salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX4__AajAbNa",
        "outputId": "a7e99729-7816-438c-a408-bb05fdf113bb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+------+\n",
            "|EmpID| Name|Department|Salary|\n",
            "+-----+-----+----------+------+\n",
            "|    1|Alice|     Sales|  3000|\n",
            "|    2|  Bob|        IT|  4000|\n",
            "|    3|Cathy|        HR|  3500|\n",
            "|    4|David|     Sales|  4500|\n",
            "|    5|  Eva|        IT|  4200|\n",
            "+-----+-----+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CONVERT TO RDD\n",
        "rdd = df.rdd\n",
        "print(\"RDD Example:\" , rdd.map(lambda x: (x.Name,x.Salary)).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxQNFEO6CTRF",
        "outputId": "86d95b4b-6724-496d-db2f-7529f623f69d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD Example: [('Alice', 3000), ('Bob', 4000), ('Cathy', 3500), ('David', 4500), ('Eva', 4200)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"employees\")\n",
        "str_sql = \"SELECT Name, Salary FROM employees WHERE Salary > 3500\"\n",
        "sql_res = spark.sql(str_sql)\n",
        "sql_res.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0UpQVotYCxu6",
        "outputId": "1db834f9-393b-4876-d2b7-0c2781e4f30a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| Name|Salary|\n",
            "+-----+------+\n",
            "|  Bob|  4000|\n",
            "|David|  4500|\n",
            "|  Eva|  4200|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ues6j1v1DVwB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}